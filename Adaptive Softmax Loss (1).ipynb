{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_S = 10\n",
    "Emb_dim = 300\n",
    "Sequence_length = 50\n",
    "vocab_size = 30000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Inputs in shape of (Batch_size * Sequence_Length, Dim) and targets as (Batch_size * Sequence_length,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input = np.random.randn(B_S*Sequence_length,Emb_dim)\n",
    "target = np.random.randint(30000,size=(B_S*Sequence_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the intervals on which we are going to divide our vocabulary accordingly , This mean that the first cluster contains [15000 - 3000] vocab words and the second cluster contains [vocab_size - 15000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs = [3000,15000,30000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we divided our vocabulary into 3 parts , 0 - > 3000 , 3000 - > 15000 , 15000 - > 30000 so we need to define for each part a standalone matrix as each one has a different range of outputs. In addition , the paper proposed to use smaller dimensions for infrequent words as they wont be learned that much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_vocab_size = cutoffs[0]\n",
    "cluster1_vocab_size = cutoffs[1] - cutoffs[0]\n",
    "cluster2_vocab_size = vocab_size - cutoffs[1]\n",
    "cluster_number = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster1_dim = int((300) / (4 ** 1))\n",
    "cluster2_dim = int((300) / (4 ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_matrix = np.random.randn(Emb_dim,head_vocab_size + cluster_number)\n",
    "cluster1_matrix1 = np.random.randn(Emb_dim,cluster1_dim)\n",
    "cluster1_matrix2 = np.random.randn(cluster1_dim,cluster1_vocab_size)\n",
    "cluster2_matrix1 = np.random.randn(Emb_dim,cluster2_dim)\n",
    "cluster2_matrix2 = np.random.randn(cluster2_dim,cluster2_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster1_matrix1 = np.random.randn(Emb_dim,cluster1_dim)\n",
    "cluster1_matrix2 = np.random.randn(cluster1_dim,cluster1_vocab_size)\n",
    "cluster2_matrix1 = np.random.randn(Emb_dim,cluster2_dim)\n",
    "cluster2_matrix2 = np.random.randn(cluster2_dim,cluster2_vocab_size)\n",
    "def cluster(i,Input):\n",
    "    if i == 1:\n",
    "            Input = np.dot(Input,mat1)\n",
    "            Input = np.dot(Input,mat2)\n",
    "            return Input\n",
    "    elif i == 2 :\n",
    "            Input = np.dot(Input,mat1)\n",
    "            Input = np.dot(Input,mat2)\n",
    "            return Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cutoffs = [0] + cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "## So we are now ready to compute the losses. We have the target matrix which contains indices\n",
    "## belong to different clusters so lets see how to deal with that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26874 21391 16466   507 22674 18674 22287 24051 29095 10367 23691 14886\n",
      " 12244  6110 20319 28853  9260  8746 14306 18284 23426 29480 14299 26733\n",
      " 16507 19382 22305 10199 23307 18690  4560 12551 29480 21871 20814 10352\n",
      "  3720  4913 27695  9370 25389 24032 10075 12710 17998 20905 14119 26364\n",
      " 17139 28562  7849 24846  6464 29565  5573 29313 23856 12157  8681 29755\n",
      "  7587 19752 24238 13645  1176 14143  7560 19402 11730 23278 25255   978\n",
      " 26121  9755 22263 27574  6633  8067 28166 12929  1513 29407  3356   813\n",
      "  2964 27471  1689 19708 11114  4370 20050  7207 10128  9576 21349 11706\n",
      " 10526  9712 11260 18403 14340  6352 29654  2972  3252 13193 23005  4334\n",
      " 20181 28342 23952  8974  5054 22236  5349 10045  6033  3814 10059   542\n",
      " 11604 11029  5426 26082 16168  4334 12373 11154  9255 17833 16588 18431\n",
      "  9726  7552 13700 20960  3662 27920 25976  5727 14151 21089 26341 17256\n",
      " 23031 21076 12629 20244 22729 18267 16959 11264 23309 22453 15029  1279\n",
      "  1614 17871 19878 27342  5356  2962 16880  5129  2031  9375  4093 22463\n",
      "  1671 16173  8571 16848 23149 22776  9158  5872 26692 14053  3533 10955\n",
      "  8774 10838 18739  8372  6781  9122  6928 21772 17773   191 19757  2207\n",
      " 21166 15193  2103 19203 11689 28156 17006 20576]\n"
     ]
    }
   ],
   "source": [
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(X_Input,target):\n",
    "    # Create an empty matrix to keep track of indices and losses for each entry\n",
    "    Indices_track = np.empty((B_S*Sequence_length))\n",
    "    Output_Loss = np.empty((B_S*Sequence_length))\n",
    "    \n",
    "    # We are dealing with different numbers of clusters each indexed by certain limits , so we dont care about other indices\n",
    "    # each iteration\n",
    "    for i in range(len(new_cutoffs)-1):\n",
    "        low_boundary = new_cutoffs[i]\n",
    "        high_boundary = new_cutoffs[i+1]\n",
    "        Row_mask = (target >= low_boundary) & (target < high_boundary)\n",
    "        Current_Iteration_indices = np.argwhere(Row_mask==True)\n",
    "        Current_Iteration_indices = np.squeeze(Current_Iteration_indices.T,axis=0)\n",
    "        Current_Iteration_values = target[Row_mask]\n",
    "        \n",
    "        if i == 0:\n",
    "            # Keeping track of elements belonging to the head to compute later\n",
    "            Indices_track[Row_mask] = Current_Iteration_values\n",
    "        else:\n",
    "            # Now we are in clusters calculation , which doesnot start index from 0 , so we need to create relative indices\n",
    "             #   print(Current_Iteration_values)\n",
    "            relative_indices = Current_Iteration_values - low_boundary\n",
    "            cluster_results = cluster(i,X_Input[Current_Iteration_indices,:])\n",
    "            cluster_results = log_softmax(cluster_results)\n",
    "            for j,(val,indx) in enumerate(zip(relative_indices,Current_Iteration_indices)):\n",
    "                Output_Loss[indx] = cluster_results[j][val]\n",
    "                Indices_track[indx] = head_vocab_size + i - 1  \n",
    "    Head_output = np.dot(X_Input,head_matrix)\n",
    "    relative_results = np.empty((B_S*Sequence_length))\n",
    "    Head_output = log_softmax(Head_output)\n",
    "    for l,k in enumerate(Indices_track):\n",
    "        relative_results[l] = Head_output[l][int(k)]\n",
    "    Output_Loss += relative_results\n",
    "    Mean_Loss = np.mean(-Output_Loss)\n",
    "    return Output_Loss,Mean_Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    e_x = np.exp(x)\n",
    "    return np.log(e_x/e_x.sum(axis=1,keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
